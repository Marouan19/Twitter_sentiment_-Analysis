{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "23-R51lG2quU",
    "ExecuteTime": {
     "end_time": "2024-05-16T15:32:09.895474Z",
     "start_time": "2024-05-16T15:32:09.888379Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF\n",
    "from pyspark.ml import Pipeline\n",
    "import re\n",
    "from pyspark.ml.feature import StringIndexer, IndexToString\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Initialize a Spark session\n",
    "spark = SparkSession.builder.appName(\"TwitterSentimentAnalysis\").master('local[*]').getOrCreate()"
   ],
   "metadata": {
    "id": "K_QpF6LnEMzq",
    "ExecuteTime": {
     "end_time": "2024-05-16T15:39:10.280349Z",
     "start_time": "2024-05-16T15:39:10.177284Z"
    }
   },
   "execution_count": 49,
   "outputs": [
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 61] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mConnectionRefusedError\u001B[0m                    Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[49], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Initialize a Spark session\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m spark \u001B[38;5;241m=\u001B[39m SparkSession\u001B[38;5;241m.\u001B[39mbuilder\u001B[38;5;241m.\u001B[39mappName(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTwitterSentimentAnalysis\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mmaster(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlocal[*]\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39mgetOrCreate()\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/sql/session.py:493\u001B[0m, in \u001B[0;36mSparkSession.Builder.getOrCreate\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    491\u001B[0m session \u001B[38;5;241m=\u001B[39m SparkSession\u001B[38;5;241m.\u001B[39m_instantiatedSession\n\u001B[1;32m    492\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m session \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m session\u001B[38;5;241m.\u001B[39m_sc\u001B[38;5;241m.\u001B[39m_jsc \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 493\u001B[0m     sparkConf \u001B[38;5;241m=\u001B[39m SparkConf()\n\u001B[1;32m    494\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m key, value \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_options\u001B[38;5;241m.\u001B[39mitems():\n\u001B[1;32m    495\u001B[0m         sparkConf\u001B[38;5;241m.\u001B[39mset(key, value)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/conf.py:132\u001B[0m, in \u001B[0;36mSparkConf.__init__\u001B[0;34m(self, loadDefaults, _jvm, _jconf)\u001B[0m\n\u001B[1;32m    128\u001B[0m _jvm \u001B[38;5;241m=\u001B[39m _jvm \u001B[38;5;129;01mor\u001B[39;00m SparkContext\u001B[38;5;241m.\u001B[39m_jvm\n\u001B[1;32m    130\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _jvm \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    131\u001B[0m     \u001B[38;5;66;03m# JVM is created, so create self._jconf directly through JVM\u001B[39;00m\n\u001B[0;32m--> 132\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jconf \u001B[38;5;241m=\u001B[39m _jvm\u001B[38;5;241m.\u001B[39mSparkConf(loadDefaults)\n\u001B[1;32m    133\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_conf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    134\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    135\u001B[0m     \u001B[38;5;66;03m# JVM is not created, so store data in self._conf first\u001B[39;00m\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/py4j/java_gateway.py:1712\u001B[0m, in \u001B[0;36mJVMView.__getattr__\u001B[0;34m(self, name)\u001B[0m\n\u001B[1;32m   1709\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;241m==\u001B[39m UserHelpAutoCompletion\u001B[38;5;241m.\u001B[39mKEY:\n\u001B[1;32m   1710\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m UserHelpAutoCompletion()\n\u001B[0;32m-> 1712\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_gateway_client\u001B[38;5;241m.\u001B[39msend_command(\n\u001B[1;32m   1713\u001B[0m     proto\u001B[38;5;241m.\u001B[39mREFLECTION_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\n\u001B[1;32m   1714\u001B[0m     proto\u001B[38;5;241m.\u001B[39mREFL_GET_UNKNOWN_SUB_COMMAND_NAME \u001B[38;5;241m+\u001B[39m name \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_id \u001B[38;5;241m+\u001B[39m\n\u001B[1;32m   1715\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART)\n\u001B[1;32m   1716\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m answer \u001B[38;5;241m==\u001B[39m proto\u001B[38;5;241m.\u001B[39mSUCCESS_PACKAGE:\n\u001B[1;32m   1717\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m JavaPackage(name, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_gateway_client, jvm_id\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_id)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/py4j/java_gateway.py:1036\u001B[0m, in \u001B[0;36mGatewayClient.send_command\u001B[0;34m(self, command, retry, binary)\u001B[0m\n\u001B[1;32m   1015\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msend_command\u001B[39m(\u001B[38;5;28mself\u001B[39m, command, retry\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, binary\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[1;32m   1016\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001B[39;00m\n\u001B[1;32m   1017\u001B[0m \u001B[38;5;124;03m       called directly by Py4J users. It is usually called by\u001B[39;00m\n\u001B[1;32m   1018\u001B[0m \u001B[38;5;124;03m       :class:`JavaMember` instances.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1034\u001B[0m \u001B[38;5;124;03m     if `binary` is `True`.\u001B[39;00m\n\u001B[1;32m   1035\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 1036\u001B[0m     connection \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_connection()\n\u001B[1;32m   1037\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1038\u001B[0m         response \u001B[38;5;241m=\u001B[39m connection\u001B[38;5;241m.\u001B[39msend_command(command)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/py4j/clientserver.py:284\u001B[0m, in \u001B[0;36mJavaClient._get_connection\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    281\u001B[0m     \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[1;32m    283\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m connection \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m connection\u001B[38;5;241m.\u001B[39msocket \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 284\u001B[0m     connection \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_new_connection()\n\u001B[1;32m    285\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m connection\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/py4j/clientserver.py:291\u001B[0m, in \u001B[0;36mJavaClient._create_new_connection\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    287\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_create_new_connection\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    288\u001B[0m     connection \u001B[38;5;241m=\u001B[39m ClientServerConnection(\n\u001B[1;32m    289\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mjava_parameters, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpython_parameters,\n\u001B[1;32m    290\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_property, \u001B[38;5;28mself\u001B[39m)\n\u001B[0;32m--> 291\u001B[0m     connection\u001B[38;5;241m.\u001B[39mconnect_to_java_server()\n\u001B[1;32m    292\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mset_thread_connection(connection)\n\u001B[1;32m    293\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m connection\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/py4j/clientserver.py:438\u001B[0m, in \u001B[0;36mClientServerConnection.connect_to_java_server\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    435\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mssl_context:\n\u001B[1;32m    436\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msocket \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mssl_context\u001B[38;5;241m.\u001B[39mwrap_socket(\n\u001B[1;32m    437\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msocket, server_hostname\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mjava_address)\n\u001B[0;32m--> 438\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msocket\u001B[38;5;241m.\u001B[39mconnect((\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mjava_address, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mjava_port))\n\u001B[1;32m    439\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstream \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msocket\u001B[38;5;241m.\u001B[39mmakefile(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrb\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    440\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mis_connected \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[0;31mConnectionRefusedError\u001B[0m: [Errno 61] Connection refused"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"branch\", StringType(), True),\n",
    "    StructField(\"sentiment\", StringType(), True),\n",
    "    StructField(\"tweet\", StringType(), True)\n",
    "])\n",
    "\n",
    "training = spark.read.csv(\"twitter_training.csv\", header=True, schema=schema)\n",
    "validation = spark.read.csv(\"twitter_validation.csv\", header=True, schema=schema)\n"
   ],
   "metadata": {
    "id": "LzIdqdF5Ej4R",
    "ExecuteTime": {
     "end_time": "2024-05-16T15:32:09.992919Z",
     "start_time": "2024-05-16T15:32:09.948531Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 61] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mConnectionRefusedError\u001B[0m                    Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[47], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Stop the Spark session\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m spark\u001B[38;5;241m.\u001B[39mstop()\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/sql/session.py:1796\u001B[0m, in \u001B[0;36mSparkSession.stop\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1782\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1783\u001B[0m \u001B[38;5;124;03mStop the underlying :class:`SparkContext`.\u001B[39;00m\n\u001B[1;32m   1784\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1792\u001B[0m \u001B[38;5;124;03m>>> spark.stop()  # doctest: +SKIP\u001B[39;00m\n\u001B[1;32m   1793\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1794\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcontext\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SQLContext\n\u001B[0;32m-> 1796\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sc\u001B[38;5;241m.\u001B[39mstop()\n\u001B[1;32m   1797\u001B[0m \u001B[38;5;66;03m# We should clean the default session up. See SPARK-23228.\u001B[39;00m\n\u001B[1;32m   1798\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jvm \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/context.py:654\u001B[0m, in \u001B[0;36mSparkContext.stop\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    652\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_jsc\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m    653\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 654\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jsc\u001B[38;5;241m.\u001B[39mstop()\n\u001B[1;32m    655\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m Py4JError:\n\u001B[1;32m    656\u001B[0m         \u001B[38;5;66;03m# Case: SPARK-18523\u001B[39;00m\n\u001B[1;32m    657\u001B[0m         warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[1;32m    658\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnable to cleanly shutdown Spark JVM process.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    659\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m It is possible that the process has crashed,\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    660\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m been killed or may also be in a zombie state.\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    661\u001B[0m             \u001B[38;5;167;01mRuntimeWarning\u001B[39;00m,\n\u001B[1;32m    662\u001B[0m         )\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1314\u001B[0m args_command, temp_args \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_args(\u001B[38;5;241m*\u001B[39margs)\n\u001B[1;32m   1316\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1319\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[0;32m-> 1321\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[1;32m   1322\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n\u001B[1;32m   1323\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n\u001B[1;32m   1325\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/py4j/java_gateway.py:1036\u001B[0m, in \u001B[0;36mGatewayClient.send_command\u001B[0;34m(self, command, retry, binary)\u001B[0m\n\u001B[1;32m   1015\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msend_command\u001B[39m(\u001B[38;5;28mself\u001B[39m, command, retry\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, binary\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[1;32m   1016\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001B[39;00m\n\u001B[1;32m   1017\u001B[0m \u001B[38;5;124;03m       called directly by Py4J users. It is usually called by\u001B[39;00m\n\u001B[1;32m   1018\u001B[0m \u001B[38;5;124;03m       :class:`JavaMember` instances.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1034\u001B[0m \u001B[38;5;124;03m     if `binary` is `True`.\u001B[39;00m\n\u001B[1;32m   1035\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 1036\u001B[0m     connection \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_connection()\n\u001B[1;32m   1037\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1038\u001B[0m         response \u001B[38;5;241m=\u001B[39m connection\u001B[38;5;241m.\u001B[39msend_command(command)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/py4j/clientserver.py:284\u001B[0m, in \u001B[0;36mJavaClient._get_connection\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    281\u001B[0m     \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[1;32m    283\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m connection \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m connection\u001B[38;5;241m.\u001B[39msocket \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 284\u001B[0m     connection \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_new_connection()\n\u001B[1;32m    285\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m connection\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/py4j/clientserver.py:291\u001B[0m, in \u001B[0;36mJavaClient._create_new_connection\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    287\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_create_new_connection\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    288\u001B[0m     connection \u001B[38;5;241m=\u001B[39m ClientServerConnection(\n\u001B[1;32m    289\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mjava_parameters, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpython_parameters,\n\u001B[1;32m    290\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_property, \u001B[38;5;28mself\u001B[39m)\n\u001B[0;32m--> 291\u001B[0m     connection\u001B[38;5;241m.\u001B[39mconnect_to_java_server()\n\u001B[1;32m    292\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mset_thread_connection(connection)\n\u001B[1;32m    293\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m connection\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/py4j/clientserver.py:438\u001B[0m, in \u001B[0;36mClientServerConnection.connect_to_java_server\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    435\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mssl_context:\n\u001B[1;32m    436\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msocket \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mssl_context\u001B[38;5;241m.\u001B[39mwrap_socket(\n\u001B[1;32m    437\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msocket, server_hostname\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mjava_address)\n\u001B[0;32m--> 438\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msocket\u001B[38;5;241m.\u001B[39mconnect((\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mjava_address, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mjava_port))\n\u001B[1;32m    439\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstream \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msocket\u001B[38;5;241m.\u001B[39mmakefile(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrb\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    440\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mis_connected \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[0;31mConnectionRefusedError\u001B[0m: [Errno 61] Connection refused"
     ]
    }
   ],
   "source": [
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-iPS4jqv0Glu",
    "outputId": "23b55142-b2e7-4c36-e9bb-f4c619d2f484",
    "ExecuteTime": {
     "end_time": "2024-05-16T15:33:26.657574Z",
     "start_time": "2024-05-16T15:33:26.453083Z"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "training.printSchema()\n",
    "validation.printSchema()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YQU2fY6UFZj4",
    "outputId": "cb1e790d-4961-4ab5-e330-7851262f0af7",
    "ExecuteTime": {
     "start_time": "2024-05-16T15:32:09.950468Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "training.show()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y75M43IcFFe_",
    "outputId": "f3ddf823-66cf-427d-9265-d8cc57d4c9a1",
    "ExecuteTime": {
     "start_time": "2024-05-16T15:32:09.952598Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "validation.show()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uaRo6VrkFN6X",
    "outputId": "d53f9247-f25c-4947-f5d7-64b9c55e17a3",
    "ExecuteTime": {
     "start_time": "2024-05-16T15:32:09.954403Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "wUODSJDjFrWb",
    "ExecuteTime": {
     "start_time": "2024-05-16T15:32:09.956140Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Group data by sentiment and count the occurrences\n",
    "sentiment_counts = training.groupBy('sentiment').count().orderBy('sentiment')\n",
    "\n",
    "# Collect data to the driver\n",
    "sentiment_counts_collect = sentiment_counts.collect()\n",
    "\n",
    "# Extract sentiment labels and counts\n",
    "sentiments = [row['sentiment'] for row in sentiment_counts_collect]\n",
    "counts = [row['count'] for row in sentiment_counts_collect]\n",
    "\n",
    "# Plotting\n",
    "plt.barh(sentiments, counts, color=sns.color_palette('Dark2'))\n",
    "plt.gca().spines[['top', 'right']].set_visible(False)\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Sentiment')\n",
    "plt.title('Sentiment Distribution')\n",
    "plt.show()\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "AgFGT8CpGQk6",
    "outputId": "2f810a74-1611-4ce6-ed86-8b01bfc07ebc",
    "ExecuteTime": {
     "start_time": "2024-05-16T15:32:09.957920Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import sum, col\n",
    "\n",
    "# Count null values in each column\n",
    "missing_values1 = training.agg(*[sum(col(c).isNull().cast(\"int\")).alias(c) for c in training.columns])\n",
    "\n",
    "# Show the result\n",
    "missing_values1.show()\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Dj69dWSaGw3J",
    "outputId": "cccfee65-a05a-467e-f009-e499eaf75f33",
    "ExecuteTime": {
     "start_time": "2024-05-16T15:32:09.960321Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Drop rows with any null values\n",
    "training = training.na.drop()\n",
    "\n",
    "# Show the updated DataFrame\n",
    "training.show()\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rC9K3-GqHqqp",
    "outputId": "47209643-ebbc-49c1-b6a3-89021d51dece",
    "ExecuteTime": {
     "start_time": "2024-05-16T15:32:09.962400Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate sentiment distribution\n",
    "sentiment_distribution = training.groupBy('sentiment').count().orderBy('sentiment').toPandas()\n",
    "\n",
    "# Define colors\n",
    "colors = ['red', 'green', 'blue', 'gray']\n",
    "\n",
    "# Create subplots\n",
    "fig, axs = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plotting pie chart for sentiment distribution with custom colors\n",
    "axs[0].pie(sentiment_distribution['count'], labels=sentiment_distribution['sentiment'], autopct='%1.1f%%',\n",
    "            startangle=90, wedgeprops={'linewidth': 0.5}, textprops={'fontsize': 12},\n",
    "            explode=[0.1, 0.1, 0.1, 0.1], colors=colors, shadow=True)\n",
    "axs[0].set_title('Sentiment Distribution - Pie Chart')\n",
    "\n",
    "# Plotting bar plot for sentiment distribution\n",
    "axs[1].bar(sentiment_distribution['sentiment'], sentiment_distribution['count'], color=colors)\n",
    "axs[1].set_title('Sentiment Distribution - Bar Plot')\n",
    "axs[1].set_xlabel('Sentiment')\n",
    "axs[1].set_ylabel('Count')\n",
    "axs[1].tick_params(axis='x', rotation=45)\n",
    "axs[1].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "# Add text on top of each bar in the bar plot\n",
    "for i, count in enumerate(sentiment_distribution['count']):\n",
    "    axs[1].text(i, count + 0.5, str(count), ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "id": "E79mf28LIZNO",
    "outputId": "f9a9e636-c1f5-4999-8c45-8a31b8657c35",
    "ExecuteTime": {
     "start_time": "2024-05-16T15:32:09.964703Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Filter out rows with non-string values in the specified column\n",
    "def filter_non_string(df, column):\n",
    "    df = df.filter(df[column].isNotNull())\n",
    "    df = df.withColumn(column, df[column].cast(StringType()))\n",
    "    return df"
   ],
   "metadata": {
    "id": "z3226UnQK18r",
    "ExecuteTime": {
     "start_time": "2024-05-16T15:32:09.966534Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Convert text to lowercase\n",
    "def normalize_text(text):\n",
    "    return text.lower()"
   ],
   "metadata": {
    "id": "Kszj9AC5K4Qr",
    "ExecuteTime": {
     "start_time": "2024-05-16T15:32:09.968317Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Remove HTML tags from the text\n",
    "def remove_html_tags(text):\n",
    "    return re.sub(r'<.*?>', '', text)"
   ],
   "metadata": {
    "id": "6AUZEczyK6ly",
    "ExecuteTime": {
     "start_time": "2024-05-16T15:32:09.970339Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Remove URLs or hyperlinks from the text\n",
    "def remove_urls(text):\n",
    "    return re.sub(r'http\\S+|www\\S+', '', text)"
   ],
   "metadata": {
    "id": "LSlOR4dVK8pk",
    "ExecuteTime": {
     "start_time": "2024-05-16T15:32:09.971809Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Exclude numerical digits from the text\n",
    "def remove_numbers(text):\n",
    "    return re.sub(r'\\d+', '', text)"
   ],
   "metadata": {
    "id": "7y5PBd-LK-gO",
    "ExecuteTime": {
     "start_time": "2024-05-16T15:32:09.973317Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Remove punctuation marks from the text\n",
    "def remove_punctuation(text):\n",
    "    return re.sub(r'[^\\w\\s]', '', text)"
   ],
   "metadata": {
    "id": "_c1bTEHjLAjv",
    "ExecuteTime": {
     "start_time": "2024-05-16T15:32:09.975044Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Remove emojis from the text\n",
    "def remove_emojis(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u\"\\U00010000-\\U0010ffff\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u200d\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\ufe0f\"  # dingbats\n",
    "                               u\"\\u3030\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n"
   ],
   "metadata": {
    "id": "REH3bFLAJLZ6",
    "ExecuteTime": {
     "start_time": "2024-05-16T15:32:09.977082Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from pyspark.ml.feature import RegexTokenizer\n",
    "\n",
    "def tokenize_text(dataframe, input_col, output_col):\n",
    "    \"\"\"Tokenize the text column in the given DataFrame.\"\"\"\n",
    "    tokenizer = RegexTokenizer(inputCol=input_col, outputCol=output_col, pattern=\"\\\\W\")\n",
    "    return tokenizer.transform(dataframe)"
   ],
   "metadata": {
    "id": "107Nf2pujqd_",
    "ExecuteTime": {
     "start_time": "2024-05-16T15:32:09.978140Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    \"\"\"Eliminate common stopwords from the tokenized text using PySpark.\"\"\"\n",
    "    remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered_tokens\")\n",
    "    return remover.transform(tokens).select(\"filtered_tokens\")"
   ],
   "metadata": {
    "id": "r6CM_we_j9-h",
    "ExecuteTime": {
     "start_time": "2024-05-16T15:32:09.979171Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.feature import IDF\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType\n",
    "\n",
    "def vectorize_data(text_data):\n",
    "    \"\"\"Vectorize the tokenized text data using TF-IDF in PySpark.\"\"\"\n",
    "    # Join the tokenized text into strings\n",
    "    text_data_strings = [\" \".join(tokens) for tokens in text_data]\n",
    "\n",
    "    # Create DataFrame from text data\n",
    "    text_df = spark.createDataFrame(zip(range(len(text_data_strings)), text_data_strings), [\"id\", \"text\"])\n",
    "\n",
    "    # Tokenize text\n",
    "    tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"tokens\")\n",
    "    tokenized_df = tokenizer.transform(text_df)\n",
    "\n",
    "    # Count Vectorizer\n",
    "    cv = CountVectorizer(inputCol=\"tokens\", outputCol=\"raw_features\")\n",
    "    cv_model = cv.fit(tokenized_df)\n",
    "    count_vectorized_df = cv_model.transform(tokenized_df)\n",
    "\n",
    "    # Compute IDF\n",
    "    idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
    "    idf_model = idf.fit(count_vectorized_df)\n",
    "    tfidf_vectorized_df = idf_model.transform(count_vectorized_df)\n",
    "\n",
    "    # Select only the features column\n",
    "    select_features_udf = udf(lambda features: features.toArray().tolist(), ArrayType())\n",
    "    return tfidf_vectorized_df.select(\"id\", select_features_udf(\"features\")).withColumnRenamed(\"<lambda>(features)\", \"tfidf_vectors\"), cv_model\n"
   ],
   "metadata": {
    "id": "i0BCsnAvkN0o",
    "ExecuteTime": {
     "start_time": "2024-05-16T15:32:09.980665Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Define UDFs for preprocessing steps\n",
    "normalize_text_udf = udf(normalize_text, StringType())\n",
    "remove_html_tags_udf = udf(remove_html_tags, StringType())\n",
    "remove_urls_udf = udf(remove_urls, StringType())\n",
    "remove_numbers_udf = udf(remove_numbers, StringType())\n",
    "remove_punctuation_udf = udf(remove_punctuation, StringType())\n",
    "remove_emojis_udf = udf(remove_emojis, StringType())\n",
    "\n",
    "# Preprocess text\n",
    "def preprocess_text(df, column):\n",
    "    df = filter_non_string(df, column)\n",
    "    df = df.withColumn(column, normalize_text_udf(col(column)))\n",
    "    df = df.withColumn(column, remove_html_tags_udf(col(column)))\n",
    "    df = df.withColumn(column, remove_urls_udf(col(column)))\n",
    "    df = df.withColumn(column, remove_numbers_udf(col(column)))\n",
    "    df = df.withColumn(column, remove_punctuation_udf(col(column)))\n",
    "    df = df.withColumn(column, remove_emojis_udf(col(column)))\n",
    "    return df\n",
    "\n",
    "# Usage:\n",
    "df1_processed = preprocess_text(training, 'tweet')"
   ],
   "metadata": {
    "id": "w87Rc3ZwKgdW",
    "ExecuteTime": {
     "start_time": "2024-05-16T15:32:09.982466Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "\n",
    "def preprocess_text(df):\n",
    "    # Filter non-string values in the 'tweet' column\n",
    "    df = filter_non_string(df, 'tweet')\n",
    "    # Define UDFs for text preprocessing steps\n",
    "    normalize_text_udf = udf(normalize_text, StringType())\n",
    "    remove_html_tags_udf = udf(remove_html_tags, StringType())\n",
    "    remove_urls_udf = udf(remove_urls, StringType())\n",
    "    remove_numbers_udf = udf(remove_numbers, StringType())\n",
    "    remove_punctuation_udf = udf(remove_punctuation, StringType())\n",
    "    remove_emojis_udf = udf(remove_emojis, StringType())\n",
    "\n",
    "    # Apply text preprocessing steps using DataFrame transformations\n",
    "    df = df.withColumn(\"tweet\", normalize_text_udf(\"tweet\"))\n",
    "    df = df.withColumn(\"tweet\", remove_html_tags_udf(\"tweet\"))\n",
    "    df = df.withColumn(\"tweet\", remove_urls_udf(\"tweet\"))\n",
    "    df = df.withColumn(\"tweet\", remove_numbers_udf(\"tweet\"))\n",
    "    df = df.withColumn(\"tweet\", remove_punctuation_udf(\"tweet\"))\n",
    "\n",
    "    # Tokenize text\n",
    "    tokenizer = Tokenizer(inputCol=\"tweet\", outputCol=\"tokens\")\n",
    "    df = tokenizer.transform(df)\n",
    "\n",
    "    # Remove stopwords\n",
    "    remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered_tokens\")\n",
    "    df = remover.transform(df).select(\"branch\",\"tweet\",'sentiment', \"filtered_tokens\")\n",
    "\n",
    "\n",
    "    return df\n",
    "\n",
    "# Assuming 'training' is your Spark DataFrame\n",
    "df_processed = preprocess_text(training)\n"
   ],
   "metadata": {
    "id": "RJNwEPNUkjXL",
    "ExecuteTime": {
     "start_time": "2024-05-16T15:32:09.983699Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df_processed.show()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YUFv9EvXim_d",
    "outputId": "a25fdef6-3451-43d3-b0c0-4b5f420b64bd",
    "ExecuteTime": {
     "start_time": "2024-05-16T15:32:09.985016Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import concat_ws\n",
    "\n",
    "# Convert list of words into a single string for each entry in 'tweet' column\n",
    "df_processed = df_processed.withColumn(\"tweet\", concat_ws(\" \", \"filtered_tokens\"))\n",
    "\n",
    "# Show the updated DataFrame\n",
    "df_processed.show()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2_j3y_30KrFP",
    "outputId": "81cb8295-1fc8-469c-9e48-c125491431ca",
    "ExecuteTime": {
     "start_time": "2024-05-16T15:32:09.986393Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df_processed.show()\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8CM2SEhjn4Kw",
    "outputId": "3968f721-9c7f-45c6-8939-e59e878ee95f",
    "ExecuteTime": {
     "start_time": "2024-05-16T15:32:09.987693Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Split data into training and testing sets\n",
    "train_df, test_df = df_processed.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Assuming 'sentiment' column is your label column\n",
    "indexer = StringIndexer(inputCol=\"sentiment\", outputCol=\"label\")\n",
    "train_df = indexer.fit(train_df).transform(train_df)\n",
    "test_df = indexer.fit(test_df).transform(test_df)\n",
    "\n",
    "# Convert text data to TF-IDF features\n",
    "hashingTF = HashingTF(inputCol=\"filtered_tokens\", outputCol=\"raw_features\", numFeatures=10000)\n",
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", maxIter=10, regParam=0.01)\n",
    "\n",
    "pipeline = Pipeline(stages=[hashingTF, idf, lr])\n",
    "\n",
    "# Train Logistic Regression classifier\n",
    "lr_model = pipeline.fit(train_df)\n",
    "\n",
    "# Predict on the testing data\n",
    "predictions = lr_model.transform(test_df)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"label\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Accuracy:\", accuracy)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rUnDkVBQoZjZ",
    "outputId": "cfd1d2eb-b581-48f6-c796-701f5b005171",
    "ExecuteTime": {
     "start_time": "2024-05-16T15:32:09.989468Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from pyspark.ml.feature import HashingTF, CountVectorizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Split data into training and testing sets\n",
    "train_df, test_df = df_processed.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Assuming 'sentiment' column is your label column\n",
    "indexer = StringIndexer(inputCol=\"sentiment\", outputCol=\"label\")\n",
    "train_df = indexer.fit(train_df).transform(train_df)\n",
    "test_df = indexer.fit(test_df).transform(test_df)\n",
    "\n",
    "# Convert text data to Bag of Words features\n",
    "# Using HashingTF\n",
    "# hashingTF = HashingTF(inputCol=\"filtered_tokens\", outputCol=\"raw_features\", numFeatures=10000)\n",
    "# Using CountVectorizer\n",
    "cv = CountVectorizer(inputCol=\"filtered_tokens\", outputCol=\"features\", vocabSize=50000)\n",
    "\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", maxIter=1000, regParam=0.01)\n",
    "\n",
    "pipeline = Pipeline(stages=[cv, lr])\n",
    "\n",
    "# Train Logistic Regression classifier\n",
    "lr_model = pipeline.fit(train_df)\n",
    "\n",
    "# Predict on the testing data\n",
    "predictions = lr_model.transform(test_df)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"label\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Accuracy:\", accuracy)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pXvFcL4e4LPV",
    "outputId": "7ecd9728-f873-47af-a915-c049e02ec1a6",
    "ExecuteTime": {
     "start_time": "2024-05-16T15:32:09.990705Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Convert text data to TF-IDF features\n",
    "hashingTF = HashingTF(inputCol=\"filtered_tokens\", outputCol=\"raw_features\", numFeatures=10000)\n",
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
    "lr2= LogisticRegression(featuresCol='features', labelCol='label')\n",
    "\n",
    "pipeline2 = Pipeline(stages=[hashingTF,idf, lr2])\n",
    "# Fit the model on the training data\n",
    "lr2_model = pipeline2.fit(train_df)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions1 = lr_model.transform(test_df)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator1 = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy1 = evaluator1.evaluate(predictions1)\n",
    "\n",
    "print(\"Logistic Regression Accuracy: %f\" % accuracy)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yf76D8rd3lYD",
    "outputId": "c8e464ed-30d9-49c2-f33d-effd69e077a3",
    "ExecuteTime": {
     "start_time": "2024-05-16T15:32:09.991865Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Save the model\n",
    "lrmodel.save(\"lrmodel\")\n"
   ],
   "metadata": {
    "id": "wm4qc1w-9MFU",
    "ExecuteTime": {
     "end_time": "2024-05-16T15:32:10.028664Z",
     "start_time": "2024-05-16T15:32:09.993048Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Save the model\n",
    "lrModel.save(\"lrmodel\")\n"
   ],
   "metadata": {
    "id": "yXhaGyqQqkFm",
    "ExecuteTime": {
     "start_time": "2024-05-16T15:32:09.994202Z"
    }
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
